"""
Depth Estimation with Fallback Support
---------------------------------------
- Primary: Depth Anything V2 (if available)
- Fallback: ZoeDepth for Railway compatibility
- CPU/GPU optimization support  
- Memory management for constrained environments
- Colormap visualization for depth maps

Usage:
  estimator = DepthEstimator(DepthConfig())
  depth_path = await estimator.estimate_depth(image_path)
"""

import os
import asyncio
import tempfile
import logging
from pathlib import Path
from typing import Optional, Dict, Any, Tuple
from dataclasses import dataclass

import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image
import cv2
import matplotlib
matplotlib.use('Agg')  # Non-interactive backend
import matplotlib.pyplot as plt
import matplotlib.cm as cm

import structlog

# Configure logging
logger = structlog.get_logger(__name__)

@dataclass
class DepthConfig:
    """Configuration for depth estimation"""
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    cpu_optimization: bool = True
    reduce_precision: bool = False  # Use float16 for GPU memory savings
    model_name: str = "depth-anything/Depth-Anything-V2-Large"  # Depth Anything V2 model
    max_image_size: int = 518  # Optimal input size for Depth Anything V2
    colormap: str = "inferno"  # Colormap for visualization

class DepthEstimator:
    """Depth Anything V2 depth estimator with optimizations"""
    
    def __init__(self, config: DepthConfig):
        self.config = config
        self.model = None
        self.processor = None
        self.is_available = False
        self.model_type = "none"
        
        logger.info(f"üîç Initializing DepthEstimator on {config.device}")
        self._setup_model()
    
    def _setup_model(self):
        """Initialize depth model with fallback support"""
        
        # Try Depth Anything V2 first
        try:
            logger.info("üéØ Attempting to initialize Depth Anything V2...")
            self._setup_depth_anything_v2()
            self.model_type = "depth_anything_v2"
            self.is_available = True
            logger.info(f"‚úÖ Depth Anything V2 initialized successfully on {self.config.device}")
            return
        except ImportError as e:
            logger.warning(f"‚ö†Ô∏è Depth Anything V2 not available: {e}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to initialize Depth Anything V2: {e}")
        
        # Fallback to MiDaS (Python 3.12+ compatible)
        try:
            logger.info("üîÑ Falling back to MiDaS (Python 3.12+ compatible)...")
            self._setup_midas()
            self.model_type = "midas"
            self.is_available = True
            logger.info(f"‚úÖ MiDaS initialized successfully on {self.config.device}")
            return
        except ImportError as e:
            logger.warning(f"‚ö†Ô∏è MiDaS not available: {e}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to initialize MiDaS: {e}")
        
        # No depth model available
        logger.error("‚ùå No depth estimation model available")
        logger.error("üí° Install one of:")
        logger.error("   - Depth Anything V2: pip install git+https://github.com/badayvedat/Depth-Anything-V2.git@badayvedat-patch-1")
        logger.error("   - MiDaS: pip install timm (should be available with transformers)")
        self.is_available = False
        self.model_type = "none"
    
    def _setup_depth_anything_v2(self):
        """Initialize Depth Anything V2 model"""
        # Import Depth Anything V2
        from depth_anything_v2.dpt import DepthAnythingV2
        
        # Model configurations from official repo
        model_configs = {
            'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},
            'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},
            'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},
            'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}
        }
        
        # Use Large model for best quality (vitl)
        encoder = 'vitl'
        self.model = DepthAnythingV2(**model_configs[encoder])
        
        # Try to load pretrained weights
        model_path = self._download_model(encoder)
        if model_path and os.path.exists(model_path):
            checkpoint = torch.load(model_path, map_location='cpu')
            self.model.load_state_dict(checkpoint)
            logger.info("‚úÖ Loaded Depth Anything V2 pretrained weights")
        else:
            logger.warning("‚ö†Ô∏è Using Depth Anything V2 without pretrained weights")
        
        # Move to device and set precision
        self.model = self.model.to(self.config.device).eval()
        if self.config.reduce_precision and self.config.device == "cuda":
            self.model = self.model.half()
            logger.info("üîß Using half precision for GPU memory savings")
        
        # CPU optimizations
        if self.config.device == "cpu" and self.config.cpu_optimization:
            torch.set_num_threads(min(4, torch.get_num_threads()))
            logger.info("üîß CPU optimizations enabled")
    
    def _setup_midas(self):
        """Initialize MiDaS model as Python 3.12+ compatible fallback"""
        from transformers import pipeline
        
        # Load MiDaS model via transformers pipeline (Python 3.12+ compatible)
        logger.info("üì¶ Loading MiDaS model via transformers...")
        self.model = pipeline(
            "depth-estimation",
            model="Intel/dpt-large",
            device=0 if self.config.device == "cuda" and torch.cuda.is_available() else -1
        )
        
        logger.info("üîß MiDaS model loaded via transformers pipeline")
    
    def _download_model(self, encoder: str) -> Optional[str]:
        """Download Depth Anything V2 model weights"""
        try:
            # Create model cache directory
            cache_dir = os.getenv("MODEL_CACHE_DIR", "/tmp/depth_anything_v2")
            os.makedirs(cache_dir, exist_ok=True)
            
            model_file = os.path.join(cache_dir, f"depth_anything_v2_{encoder}.pth")
            
            if not os.path.exists(model_file):
                logger.info(f"üì¶ Downloading Depth Anything V2 {encoder} model weights...")
                import urllib.request
                
                # Official Hugging Face model URLs
                model_urls = {
                    'vits': 'https://huggingface.co/depth-anything/Depth-Anything-V2-Small/resolve/main/depth_anything_v2_vits.pth',
                    'vitb': 'https://huggingface.co/depth-anything/Depth-Anything-V2-Base/resolve/main/depth_anything_v2_vitb.pth',
                    'vitl': 'https://huggingface.co/depth-anything/Depth-Anything-V2-Large/resolve/main/depth_anything_v2_vitl.pth',
                    'vitg': 'https://huggingface.co/depth-anything/Depth-Anything-V2-Giant/resolve/main/depth_anything_v2_vitg.pth'
                }
                
                if encoder in model_urls:
                    urllib.request.urlretrieve(model_urls[encoder], model_file)
                    logger.info("‚úÖ Model weights downloaded successfully")
                else:
                    logger.error(f"‚ùå Unknown encoder: {encoder}")
                    return None
            
            return model_file
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to download model weights: {e}")
            return None
    
    # Preprocessing and postprocessing handled internally by Depth Anything V2
    
    def _create_depth_visualization(self, depth_array: np.ndarray, output_path: str) -> bool:
        """Create colorized depth map visualization"""
        try:
            # Apply colormap
            colormap = cm.get_cmap(self.config.colormap)
            colored_depth = colormap(depth_array)
            
            # Convert to 8-bit RGB
            colored_depth_rgb = (colored_depth[:, :, :3] * 255).astype(np.uint8)
            
            # Save as PNG
            depth_image = Image.fromarray(colored_depth_rgb, 'RGB')
            depth_image.save(output_path, 'PNG', optimize=True)
            
            logger.info(f"‚úÖ Depth visualization saved: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to create depth visualization: {e}")
            return False
    
    async def estimate_depth(self, image_path: str, output_dir: Optional[str] = None) -> Optional[str]:
        """
        Estimate depth for an image using Depth Anything V2
        
        Args:
            image_path: Path to input image
            output_dir: Output directory for depth map (optional)
            
        Returns:
            Path to generated depth map or None if failed
        """
        if not self.is_available:
            logger.error("‚ùå Depth estimator not available")
            return None
        
        try:
            # Load and validate image
            if not os.path.exists(image_path):
                logger.error(f"‚ùå Image not found: {image_path}")
                return None
            
            image = Image.open(image_path)
            logger.info(f"üñºÔ∏è Processing image: {image.size}")
            
            # Run depth estimation based on model type
            with torch.no_grad():
                if self.model_type == "depth_anything_v2":
                    # Convert PIL image to OpenCV format (Depth Anything V2 expects BGR)
                    image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
                    # Use the model's infer_image method
                    depth_array = self.model.infer_image(image_cv)
                elif self.model_type == "midas":
                    # MiDaS via transformers pipeline expects PIL image
                    result = self.model(image)
                    depth_array = result["depth"]
                    # Convert to numpy if needed
                    if hasattr(depth_array, 'numpy'):
                        depth_array = depth_array.numpy()
                    elif torch.is_tensor(depth_array):
                        depth_array = depth_array.cpu().numpy()
                else:
                    logger.error(f"‚ùå Unknown model type: {self.model_type}")
                    return None
            
            # Normalize depth array to 0-1 range for visualization
            depth_normalized = (depth_array - depth_array.min()) / (depth_array.max() - depth_array.min())
            
            # Create output path
            if output_dir is None:
                output_dir = tempfile.gettempdir()
            
            os.makedirs(output_dir, exist_ok=True)
            
            # Generate filename
            base_name = Path(image_path).stem
            output_path = os.path.join(output_dir, f"{base_name}_depth.png")
            
            # Create and save visualization
            if self._create_depth_visualization(depth_normalized, output_path):
                logger.info(f"‚úÖ Depth estimation completed: {output_path}")
                return output_path
            else:
                return None
            
        except Exception as e:
            logger.error(f"‚ùå Depth estimation failed: {e}")
            return None
        finally:
            # Cleanup GPU memory
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    def get_info(self) -> Dict[str, Any]:
        """Get depth estimator information"""
        return {
            "model": "Depth Anything V2",
            "version": "Large (ViT-L)",
            "device": self.config.device,
            "available": self.is_available,
            "input_size": self.config.max_image_size,
            "colormap": self.config.colormap,
            "cpu_optimized": self.config.cpu_optimization,
            "precision": "half" if self.config.reduce_precision else "full"
        }