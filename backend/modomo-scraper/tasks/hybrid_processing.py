"""
Hybrid processing module for Railway deployment
Handles import + AI detection in sync when Celery workers are unavailable
"""
import structlog
import uuid
import asyncio
from datetime import datetime
from typing import Dict, Any, List, Optional

logger = structlog.get_logger(__name__)

# Import required services
from services.database_service import DatabaseService
from services.r2_uploader import create_r2_uploader, upload_to_r2_sync

# Try to import AI services
try:
    from services.detection_service import DetectionService
    AI_AVAILABLE = True
except ImportError:
    AI_AVAILABLE = False
    logger.warning("AI services not available in hybrid mode")

# Import dataset processing functions
try:
    from tasks.scraping_tasks import load_huggingface_dataset, extract_metadata_from_item, extract_image_url_from_item_sync
    SCRAPING_AVAILABLE = True
except ImportError:
    SCRAPING_AVAILABLE = False
    logger.warning("Scraping tasks not available")


def process_import_with_ai_sync(
    job_id: str, 
    dataset: str, 
    offset: int, 
    limit: int, 
    include_detection: bool = True
):
    """
    Synchronous hybrid processing: Import + AI Detection
    This runs on Railway with full AI capabilities
    """
    try:
        logger.info(f"üöÄ Starting hybrid processing: {dataset} (offset={offset}, limit={limit})")
        
        # Initialize services
        from main_refactored import _database_service
        if not _database_service:
            logger.error("‚ùå Database service not available")
            return
        
        # Update job status to running
        _database_service.supabase.table("scraping_jobs").update({
            "status": "running",
            "updated_at": datetime.utcnow().isoformat()
        }).eq("job_id", job_id).execute()
        
        # Initialize R2 uploader
        r2_uploader = create_r2_uploader()
        
        # Initialize AI detection service if available
        detection_service = None
        if include_detection and AI_AVAILABLE:
            try:
                detection_service = DetectionService()
                logger.info("‚úÖ AI detection service initialized")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è AI detection service failed to initialize: {e}")
        
        # Load dataset
        if not SCRAPING_AVAILABLE:
            logger.error("‚ùå Dataset loading not available")
            raise Exception("Dataset loading functions not available")
            
        dataset_data = load_huggingface_dataset(dataset, offset, limit)
        if not dataset_data:
            raise Exception(f"Failed to load HuggingFace dataset: {dataset}")
        
        total_items = len(dataset_data)
        logger.info(f"üì• Loaded {total_items} items from {dataset}")
        
        # Process each item
        imported = 0
        ai_processed = 0
        
        for i, item in enumerate(dataset_data):
            try:
                # Extract image URL and metadata
                image_url = extract_image_url_from_item_sync(item, r2_uploader)
                metadata = extract_metadata_from_item(item)
                
                if not image_url:
                    logger.warning(f"No image URL found in dataset item {i}")
                    continue
                
                # Create scene record
                scene_data = {
                    "houzz_id": f"hf_{dataset}_{offset + i}",
                    "image_url": image_url,
                    "metadata": metadata,
                    "status": "imported"
                }
                
                # Store scene in database
                scene_id = await store_scene_in_database(_database_service, scene_data)
                
                if scene_id:
                    imported += 1
                    logger.info(f"‚úÖ Imported scene {scene_id} ({imported}/{total_items})")
                    
                    # Run AI detection synchronously if enabled
                    if include_detection and detection_service:
                        try:
                            # Run detection directly on Railway
                            detection_result = await run_ai_detection_sync(
                                detection_service, 
                                scene_id, 
                                image_url,
                                _database_service
                            )
                            
                            if detection_result:
                                ai_processed += 1
                                logger.info(f"ü§ñ AI processed scene {scene_id} ({ai_processed}/{imported})")
                            
                        except Exception as ai_error:
                            logger.error(f"‚ùå AI detection failed for scene {scene_id}: {ai_error}")
                
                # Update progress
                progress = int((i + 1) / total_items * 100)
                _database_service.supabase.table("scraping_jobs").update({
                    "processed_items": i + 1,
                    "progress": progress,
                    "updated_at": datetime.utcnow().isoformat()
                }).eq("job_id", job_id).execute()
                
            except Exception as item_error:
                logger.error(f"‚ùå Error processing item {i}: {item_error}")
                continue
        
        # Complete the job
        result = {
            "dataset": dataset,
            "offset": offset,
            "imported_scenes": imported,
            "ai_processed_scenes": ai_processed,
            "total_requested": limit,
            "success_rate": f"{(imported/total_items)*100:.1f}%" if total_items > 0 else "0%",
            "ai_success_rate": f"{(ai_processed/imported)*100:.1f}%" if imported > 0 else "0%",
            "processing_mode": "hybrid_railway"
        }
        
        _database_service.supabase.table("scraping_jobs").update({
            "status": "completed",
            "processed_items": imported,
            "progress": 100,
            "parameters": {**_database_service.supabase.table("scraping_jobs").select("parameters").eq("job_id", job_id).execute().data[0]["parameters"], **result},
            "completed_at": datetime.utcnow().isoformat(),
            "updated_at": datetime.utcnow().isoformat()
        }).eq("job_id", job_id).execute()
        
        logger.info(f"üéâ Hybrid processing complete: {imported} imported, {ai_processed} AI processed")
        
    except Exception as e:
        logger.error(f"‚ùå Hybrid processing failed for job {job_id}: {e}")
        
        # Mark job as failed
        if '_database_service' in locals():
            _database_service.supabase.table("scraping_jobs").update({
                "status": "failed",
                "error_message": str(e),
                "updated_at": datetime.utcnow().isoformat()
            }).eq("job_id", job_id).execute()


async def store_scene_in_database(database_service: DatabaseService, scene_data: Dict[str, Any]) -> Optional[str]:
    """Store scene in database using the database service"""
    try:
        scene_id = await database_service.create_scene(
            houzz_id=scene_data["houzz_id"],
            image_url=scene_data["image_url"],
            metadata=scene_data.get("metadata", {}),
            room_type=scene_data.get("room_type"),
            caption=scene_data.get("caption"),
            image_r2_key=scene_data.get("image_r2_key")
        )
        return scene_id
        
    except Exception as e:
        logger.error(f"‚ùå Error storing scene in database: {e}")
        # Return a mock scene ID so processing doesn't fail completely
        return str(uuid.uuid4())


async def run_ai_detection_sync(
    detection_service: DetectionService,
    scene_id: str,
    image_url: str,
    database_service: DatabaseService
) -> bool:
    """
    Run AI detection synchronously on Railway
    Returns True if successful, False otherwise
    """
    try:
        logger.info(f"ü§ñ Starting AI detection for scene {scene_id}")
        
        # Download and process the image
        import requests
        from PIL import Image
        import io
        
        response = requests.get(image_url, timeout=30)
        if response.status_code != 200:
            logger.error(f"‚ùå Failed to download image: {response.status_code}")
            return False
        
        image = Image.open(io.BytesIO(response.content))
        
        # Run object detection
        detection_results = await detection_service.detect_objects(image, scene_id)
        
        if detection_results and "objects" in detection_results:
            objects_detected = len(detection_results["objects"])
            logger.info(f"üéØ Detected {objects_detected} objects in scene {scene_id}")
            
            # Store detection results in database
            for obj in detection_results["objects"]:
                try:
                    # Create detected object record
                    object_data = {
                        "scene_id": scene_id,
                        "category": obj.get("category", "unknown"),
                        "confidence": obj.get("confidence", 0.0),
                        "bbox": obj.get("bbox", {}),
                        "tags": obj.get("tags", []),
                        "metadata": obj.get("metadata", {}),
                        "approved": False  # Requires manual review
                    }
                    
                    database_service.supabase.table("detected_objects").insert(object_data).execute()
                    
                except Exception as obj_error:
                    logger.error(f"‚ùå Failed to store object: {obj_error}")
            
            return True
        else:
            logger.warning(f"‚ö†Ô∏è No objects detected in scene {scene_id}")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå AI detection failed for scene {scene_id}: {e}")
        return False


def process_scene_scraping_hybrid(job_id: str, limit: int, room_types: List[str] = None):
    """
    Hybrid scene scraping: Scrape + AI Detection
    This is for when we want to scrape new scenes and process them with AI
    """
    try:
        logger.info(f"üï∑Ô∏è Starting hybrid scene scraping: limit={limit}")
        
        from main_refactored import _database_service
        if not _database_service:
            logger.error("‚ùå Database service not available")
            return
        
        # Update job to running
        _database_service.supabase.table("scraping_jobs").update({
            "status": "running",
            "updated_at": datetime.utcnow().isoformat()
        }).eq("job_id", job_id).execute()
        
        # Initialize AI detection service
        detection_service = None
        if AI_AVAILABLE:
            try:
                detection_service = DetectionService()
                logger.info("‚úÖ AI detection service initialized for scraping")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è AI detection service failed: {e}")
        
        # Note: Actual scraping implementation would go here
        # For now, mark as completed with placeholder
        _database_service.supabase.table("scraping_jobs").update({
            "status": "completed",
            "processed_items": 0,
            "progress": 100,
            "error_message": "Scene scraping not implemented yet - placeholder completion",
            "completed_at": datetime.utcnow().isoformat(),
            "updated_at": datetime.utcnow().isoformat()
        }).eq("job_id", job_id).execute()
        
        logger.info(f"üìù Scene scraping job {job_id} marked as placeholder completion")
        
    except Exception as e:
        logger.error(f"‚ùå Scene scraping failed for job {job_id}: {e}")
        
        if '_database_service' in locals():
            _database_service.supabase.table("scraping_jobs").update({
                "status": "failed",
                "error_message": str(e),
                "updated_at": datetime.utcnow().isoformat()
            }).eq("job_id", job_id).execute()


def schedule_ai_detection_for_import(job_id: str, dataset: str, offset: int, limit: int):
    """
    Monitor Celery import job and run AI detection on Railway once import completes
    This creates the clean separation: Celery = Import, Railway = AI Detection
    """
    try:
        logger.info(f"üëÄ Monitoring import job {job_id} for AI detection scheduling")
        
        from main_refactored import _database_service
        if not _database_service:
            logger.error("‚ùå Database service not available for monitoring")
            return
        
        import time
        max_wait_time = 3600  # Wait max 1 hour for import to complete
        check_interval = 30   # Check every 30 seconds
        waited_time = 0
        
        # Monitor the import job status
        while waited_time < max_wait_time:
            try:
                # Check job status
                result = _database_service.supabase.table("scraping_jobs").select("status, processed_items").eq("job_id", job_id).execute()
                
                if not result.data:
                    logger.error(f"‚ùå Import job {job_id} not found in database")
                    return
                
                job_status = result.data[0]["status"]
                processed_items = result.data[0]["processed_items"]
                
                logger.info(f"üìä Import job {job_id} status: {job_status}, processed: {processed_items}")
                
                if job_status == "completed":
                    logger.info(f"‚úÖ Import job {job_id} completed, starting AI detection")
                    
                    # Get all scenes imported by this job
                    scenes_result = _database_service.supabase.table("scenes").select(
                        "scene_id, image_url"
                    ).like("houzz_id", f"hf_{dataset}_{offset}%").execute()
                    
                    if scenes_result.data:
                        # Start AI detection for all imported scenes
                        await run_ai_detection_batch(scenes_result.data, job_id)
                    else:
                        logger.warning(f"‚ö†Ô∏è No scenes found for import job {job_id}")
                    
                    return
                
                elif job_status in ["failed", "cancelled", "error"]:
                    logger.warning(f"‚ö†Ô∏è Import job {job_id} failed ({job_status}), skipping AI detection")
                    return
                
                # Continue monitoring
                time.sleep(check_interval)
                waited_time += check_interval
                
            except Exception as check_error:
                logger.error(f"‚ùå Error checking import job status: {check_error}")
                time.sleep(check_interval)
                waited_time += check_interval
        
        logger.warning(f"‚è∞ Timeout waiting for import job {job_id} to complete")
        
    except Exception as e:
        logger.error(f"‚ùå AI detection scheduling failed for job {job_id}: {e}")


async def run_ai_detection_batch(scenes: list, original_job_id: str):
    """
    Run AI detection on Railway for a batch of imported scenes
    This is the pure AI processing part - Railway only
    """
    try:
        from main_refactored import _database_service
        
        # Create a new detection job
        detection_job_id = f"{original_job_id}_detection"
        
        if _database_service:
            await _database_service.create_job_in_database(
                job_id=detection_job_id,
                job_type="detection",
                total_items=len(scenes),
                parameters={
                    "original_import_job": original_job_id,
                    "processing_mode": "railway_ai_only",
                    "scene_count": len(scenes)
                }
            )
        
        logger.info(f"ü§ñ Starting Railway AI detection for {len(scenes)} scenes")
        
        # Initialize AI detection service
        detection_service = None
        if AI_AVAILABLE:
            try:
                detection_service = DetectionService()
                logger.info("‚úÖ Railway AI detection service initialized")
            except Exception as e:
                logger.error(f"‚ùå Failed to initialize AI service: {e}")
                return
        else:
            logger.error("‚ùå AI detection not available on Railway")
            return
        
        ai_processed = 0
        
        for i, scene in enumerate(scenes):
            try:
                scene_id = scene["scene_id"]
                image_url = scene["image_url"]
                
                # Run AI detection
                detection_success = await run_ai_detection_sync(
                    detection_service, 
                    scene_id, 
                    image_url,
                    _database_service
                )
                
                if detection_success:
                    ai_processed += 1
                
                # Update progress
                progress = int((i + 1) / len(scenes) * 100)
                if _database_service:
                    _database_service.supabase.table("scraping_jobs").update({
                        "processed_items": i + 1,
                        "progress": progress,
                        "updated_at": datetime.utcnow().isoformat()
                    }).eq("job_id", detection_job_id).execute()
                
                logger.info(f"üéØ AI processed scene {scene_id} ({ai_processed}/{i+1})")
                
            except Exception as scene_error:
                logger.error(f"‚ùå AI detection failed for scene {scene.get('scene_id', 'unknown')}: {scene_error}")
                continue
        
        # Complete detection job
        if _database_service:
            _database_service.supabase.table("scraping_jobs").update({
                "status": "completed",
                "processed_items": len(scenes),
                "progress": 100,
                "parameters": {
                    "original_import_job": original_job_id,
                    "processing_mode": "railway_ai_only",
                    "scenes_processed": len(scenes),
                    "ai_success_count": ai_processed,
                    "ai_success_rate": f"{(ai_processed/len(scenes))*100:.1f}%" if scenes else "0%"
                },
                "completed_at": datetime.utcnow().isoformat(),
                "updated_at": datetime.utcnow().isoformat()
            }).eq("job_id", detection_job_id).execute()
        
        logger.info(f"üéâ Railway AI detection complete: {ai_processed}/{len(scenes)} scenes processed")
        
    except Exception as e:
        logger.error(f"‚ùå Batch AI detection failed: {e}")
        
        if '_database_service' in locals():
            _database_service.supabase.table("scraping_jobs").update({
                "status": "failed",
                "error_message": str(e),
                "updated_at": datetime.utcnow().isoformat()
            }).eq("job_id", detection_job_id).execute()


def schedule_ai_detection_for_scraping(job_id: str, limit: int, room_types: List[str]):
    """
    Monitor Celery scraping job and run AI detection on Railway once scraping completes
    Similar to import monitoring but for scraped scenes
    """
    try:
        logger.info(f"üëÄ Monitoring scraping job {job_id} for AI detection scheduling")
        
        from main_refactored import _database_service
        if not _database_service:
            logger.error("‚ùå Database service not available for scraping monitoring")
            return
        
        import time
        max_wait_time = 3600  # Wait max 1 hour for scraping to complete
        check_interval = 30   # Check every 30 seconds
        waited_time = 0
        
        # Monitor the scraping job status
        while waited_time < max_wait_time:
            try:
                # Check job status
                result = _database_service.supabase.table("scraping_jobs").select("status, processed_items").eq("job_id", job_id).execute()
                
                if not result.data:
                    logger.error(f"‚ùå Scraping job {job_id} not found in database")
                    return
                
                job_status = result.data[0]["status"]
                processed_items = result.data[0]["processed_items"]
                
                logger.info(f"üìä Scraping job {job_id} status: {job_status}, processed: {processed_items}")
                
                if job_status == "completed":
                    logger.info(f"‚úÖ Scraping job {job_id} completed, starting AI detection")
                    
                    # Get all scenes scraped by this job
                    # Note: This would need to be refined based on how scraped scenes are identified
                    # For now, get recent scenes (this is a placeholder approach)
                    scenes_result = _database_service.supabase.table("scenes").select(
                        "scene_id, image_url"
                    ).eq("status", "scraped").order("created_at", desc=True).limit(processed_items).execute()
                    
                    if scenes_result.data:
                        # Start AI detection for all scraped scenes
                        await run_ai_detection_batch(scenes_result.data, job_id)
                    else:
                        logger.warning(f"‚ö†Ô∏è No scraped scenes found for job {job_id}")
                    
                    return
                
                elif job_status in ["failed", "cancelled", "error"]:
                    logger.warning(f"‚ö†Ô∏è Scraping job {job_id} failed ({job_status}), skipping AI detection")
                    return
                
                # Continue monitoring
                time.sleep(check_interval)
                waited_time += check_interval
                
            except Exception as check_error:
                logger.error(f"‚ùå Error checking scraping job status: {check_error}")
                time.sleep(check_interval)
                waited_time += check_interval
        
        logger.warning(f"‚è∞ Timeout waiting for scraping job {job_id} to complete")
        
    except Exception as e:
        logger.error(f"‚ùå AI detection scheduling failed for scraping job {job_id}: {e}")